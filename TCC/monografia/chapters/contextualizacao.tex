
	\section{Contextualização}
	Atualmente, o volumes de dados que são processados e armazenados no mundo inteiro está crescendo cada vez mais, impulsionado intensamente com o aumento da capacidade de processamento e armazenamento de dados e a popularização da internet entre as pessoas. Na área da ciência surgiu um novo e quarto paradigma científico, seguindo de ciência empírica, ciência lógica e ciência computacional, chamado de \textit{data-intensive science}~\cite{hey2009}, proposto pelo cientista da computação pioneiro Jim Gray. Esse paradigma é caracterizado por focalizar na intensiva coleta e acúmulo de dados, para serem submetidos em análise a fim de extrair as informações vantajosas. Várias empresas e organizações estão dando atenção especial em \textit{big data}, um novo conceito que surgiu com aplicação do paradigma \textit{data-intensive science}, que consiste em um conjunto de dados não estruturados extremamente amplo, e por esse motivo, é impossível de ser processado por um computador convencional. O \textit{big data} é usado para analisar as relações complexas que existem entre enorme quantidade de dados contidos nele para obter as informações como previsões ou tendencias, onde geralmente são impossíveis de serem obtidas se fosse usado um volume convencional de dados. Esse crescimento de volume de dados não afetou somente áreas de negócio ou cientifico, mas também os usuários finais, com foco em armazenamento de dados, evidenciado com aumento de capacidade do disco rígido contido em um computador pessoal ou aumento de demanda por um serviço de armazenamento remoto baseando em conceitos de computação em nuvem, chamado de \textit{cloud storage service}.
	\\ 

	%Atualmente, a importância de coleta e acúmulo de dados para submeter em análise a fim de extrair as informações vantajosas está sendo discutida em diversas áreas. Este fato é evidenciado com o surgimento de , e definição de novo conceito denominado de \textit{big data}, o conjunto de dados extremamente amplo, e por esse motivo, é impossível de ser processado por um computador convencional. \\
	
	Para ter noção concreta de tamanho de dados processados, apresentamos como exemplo alguns casos da utilização de \textit{big data}. O CERN, o maior laboratório de física de partículas do mundo, que tinha anunciado em 2013 que registrou mais de 100 petabytes de dados de pesquisa ao longo dos últimos 20 anos, em que 75 petabytes destes dados foram gerado com as experiências de colisão no \textit{Large Hadron Collider} (LHC), nos últimos 3 anos~\cite{cern}. Também temos o Facebook, um serviço de rede social que em 2012 foi considerado maior do mundo, possui um \textit{data warehouse} que armazena mais de 300 petabytes de dados acumulados, e ainda 600 terabytes de dados chegam a cada dia~\cite{facebook14}. No caso de Dropbox, o serviço \textit{cloud storage} mais pupular do mundo, oferece 2 gigabytes de espaço básico para todos os usuários, podendo ser aumentado até espaço ilimitado dependendo do plano, e registrou mais de 300.000.000 usuários cadastrados~\cite{dropbox}, necessitando manter o espaço de armazenamento de no mínimo 600 petabytes disponível a qualquer instante para suprir todos seus usuários.\\
	
	Como mostra nos exemplos apresentados, um volume imenso de dados estão sendo intensamente coletado e submetido a análise. No caso do Dropbox que possui um sistema com 600 petabytes de capacidade de armazenamento estimada, implicando que será necessário, quando for usado um disco rígido com 2 terabytes de capacidade na sua composição, utilizar 300.000 dispositivos, sendo extremamente difícil de ser equipados em uma máquina que pode ser encontrada normalmente. Existe também problema com gerenciamento de dados, pois nenhum computador convencional possui o desempenho suficiente para tratamento de dados nesta escala. \\
	
	
	Uma solução tipicamente adotada  para  resolver esta questão de armazenar os dados de grande porte é aumentar igualmente a escala do sistema de armazenamento, introduzindo um sistema de arquivo distribuído.
	Esta abordagem é mais escolhida que as outras pelo fato de que apresenta uma vantagem em relação ao incremento da escala do sistema, em que neste processo o sistema distribuído possui mais facilidade e o menor custo relativo comparados com outras abordagens. 
	Um sistema distribuído consiste de um conjunto de vários computadores, chamados de nós, fisicamente distribuídos e conectados através de uma rede. 
	O nó que constitui o sistema pode ser um computador comum ou até um de grande porte, como servidor, e quando precisa aumentar o desempenho do sistema todo basta adicionar novos nós no conjunto. 
	Contudo, na abordagem de armazenamento distribuído quase sempre é  acompanhado com a questão de degradação da confiabilidade de conteúdos armazenados. 
	Como os dados que estão contidos no sistema estão localizados distribuidamente sobre vários nós, o risco da perda ou alteração de algum destes dados cresce com o aumento da escala do sistema, pois a probabilidade de ocorrer uma falha em algum dos componentes cresce proporcionalmente com o aumento da quantidade de dispositivos envolvidos.
	Para solucionar esse problema de confiabilidade dos conteúdos armazenados, muitos sistemas de arquivos distribuídos introduzem alguns mecanismos para tolerar possíveis falhas. 
	Um deles é a atribuição da redundância nos arquivos, em que são realizados principalmente com simples criação das cópias de segurança, chamadas de réplicas, para cada um dos arquivos encontrados no sistema. 
	\\
	
	A atribuição de redundância consegue impedir a queda da confiabilidade dos arquivos, mas isto tem um custo extra.
	Se um arquivo é protegido com duas réplicas significa que no sistema existem três dados idênticos, consumindo o triplo de espaço que este arquivo realmente ocupa no dispositivo de armazenamento. 
	Assim, manter a grande quantidade de dados sem degradar a segurança está tornando-se cada vez mais custoso com o aumento de volume dos dados armazenados. 
	A situação agrava ainda mais quando a ordem de tamanho chega em centenas de petabytes, como ocorre no caso de um conjunto de dados considerado como \textit{big data} ou no caso de arquivos gravados por todos os usuários de algum serviço de armazenamento nas nuvens. 
	%Para lidar com essa complexidade, uma opção é o uso da abordagem de \textit{State Machine Replication} (SMR), ou a replicação de máquina de estado, que possui o objetivo de aumentar o desempenho e a capacidade de sistema ou de prover tolerância a falha~\cite{alchieri}, criando as réplicas de computador ou servidor.\\
	
	%O uso do sistema de arquivos distribuídos resolve a questão de capacidade para armazenamento de dados de grande porte com incremento de escala, acrescentando um servidor quando for necessário. Este método é bem eficiente, prático e relativamente barato, contudo pode se tornar custoso, pois não ocorre somente adicionamento das máquinas, mas também a substituição, seja parcial ou inteira, dos componentes que apresentou algum defeito, e certamente isso ocorre com uma frequência significante e ser intenso cada vez que aumenta a escala, acumulando o custo ao longo do tempo.\\
	

	
	
	
	

 

	

