
	\section{Contextualização}
	Tradicionalmente a ciência era dividida em três paradigmas: empírica, lógica e computacional, porém o cientista computacional Jim Gray idealizou um quarto chamado de \textit{data-intensive science}~\cite{hey2009}. Tal paradigma caracteriza-se pelo intenso processamento de \textit{big data} afim de conseguir informações úteis. \textit{Big data} consiste de um conjunto extremamente amplo de dados não estruturados. Justamente pela quantidade massiva de dados que compõe o \textit{big data}, é impraticável utilizar computadores convencionais para seu processamento.
	\\
	
	Atualmente a taxa de dados processados e armazenados globalmente aumenta anualmente. Tal crescimento não foi influenciado apenas pelos estudos científicos e grandes empresas, visto que grande parcela da população criou o abito de acessar diariamente as mais diversas modalidades de redes sociais, além de utilizar as facilidades da computação nas nuvens disponibilizada por empresas como o Dropbox ou a Google. Como exemplo da popularização de tais serviços, o \textit{Dropbox} divulgou possuir mais de 300.000.000 usuários cadastrados~\cite{dropbox} em 2014, onde cada usuários dispõem um espaço mínimo de 2GB para armazenar seus arquivos remotamente, desta forma, para suprir as necessidades de seus clientes, são necessários no mínimo 600PB de espaço de armazenamento. Sobre o ramo das redes sociais, existe o caso do \textit{Facebook}, considerada em 2012 como a mario em seu ramo de atuação, cujos usuários produzem cerca de 600TB diariamente~\cite{facebook14}. Enquanto que na área de pesquisas, A Organização Europeia para a Pesquisa Nuclear (\textit{The European Organization for Nuclear Research - CERN}), o maior laboratório de física de partículas do mundo, informou em 2013 que ao longo de 20 anos produziu 100PB de dados em seus experimentos~\cite{cern}.
	\\
	
	Os casos apresentados anteriormente demonstram o imenso volume de dados que são gerados, processados e armazenados diariamente em todo o globo. Nesses casos em que o volume de dados armazenados chega na ordem de \textit{petabytes} faz-se necessário buscar formas eficientes e seguras para salvar todas as informações. Por exemplo, utilizando-se discos rígidos de 2 TB, serão necessários 50.000 dispositivos para conseguir a capacidade total de 100 PB. Tamanha quantidade de equipamentos seria impossível de serem instaladas em um único servidor, o que acarreta na necessidade de se utilizar um sistema distribuído com vários servidores, ditos como nós, trabalhando conectados através de uma rede. Desta forma, é indispensável a utilização de um sistema de arquivos distribuído (SAD) para o gerenciamento de todos os arquivos espalhados entre todos os servidores. Como exemplos reais de SAD, temos  o \textit{Google File System}, desenvolvido e utilizado pela própria \textit{Google}, O \textit{Apache Hadoop}, utilizado pelo \textit{Facebook},além do \textit{Amazon S3}, utilizado pelo \textit{Dropbox}.
	\\
	
	Visto que em um SAD os arquivos estão armazenados distribuidamente entre vários nós, a falha de um nó pode inviabilizar o uso do sistema como um todo. Tal situação ocorre pelo fato de que os arquivos armazenados pelo nó faltoso ficam inacessíveis para o restante dos nós do sistema, deste modo inviabilizando a recuperação de alguns ou até mesmo todos os dados contidos no SAD. Somado a isso, ainda temos o agravante de que a probabilidade de ocorrerem falhas em algum dos equipamentos cresce proporcionalmente com o aumento dos componentes envolvidos, desta forma, o incremento de escala pode resultar na redução da confiabilidade no sistema.
	\\
	
	É fundamental encontrar formas de contornar os problemas enfrentados pelos sistemas distribuídos, de modo a torná-lo tolerante à falhas. Para o caso de dados inacessíveis, o método de maior simplicidade é o de replicação. Na replicação os dados contidos em um nó são copiados integralmente para outros nós, assim criando cópias de segurança chamadas réplicas. Deste modo, mesmo quando algum nó apresentar problemas, o sistema ainda será capaz de recuperar qualquer arquivo que estivesse armazenado no nó defeituoso. O grande gargalo dessa abordagem é o desperdício de espaço, visto que cada réplica irá consumir a mesma quantidade de espaço de armazenamento que o arquivo original. Em sistemas que lidam com \textit{big data}, a redundância total de dados muitas vezes é impraticável.
	\\
		
	\section{Justificativa}
	No contexto dos sistemas distribuídos o conceito de paralelismo é muito presente, característica essa que acarreta no aumento de máquinas conectadas pela rede e consequentemente a probabilidade de alguma dessas máquinas sofrer erros de \textit{hardware} ou \textit{software}. Quando qualquer falha dessa natureza ocorrer, é necessário que contra medidas tenham sido implementadas a fim de minimizar os danos. Uma dessas medidas é que o sistema de arquivos distribuído seja tolerante a falhas, tolerância essa que pode ser alcançada utilizando-se a replicação total dos dados, entretanto esse \textit{overhead} ocupa espaços de armazenamento que poderiam ser utilizados para novos dados.
	\\
	
	Os conceitos de RAID podem ser utilizados a fim de aumentar o desempenho e a confiabilidade de um sistema. Durante o planejamento da tecnologia RAID vários pontos foram levados em consideração, um deles foi como aumentar a taxa de transferência da unidade de armazenamento. O modo escolhido foi utilizar o paralelismos. Com um conjunto de discos onde cada um deles executa de forma independente e paralela aos outros, é possível realizar acesso simultâneo em vários discos do conjunto, assim aumentando a vazão total de dados transferidos e resultando no aumento de desempenho do sistema. Enquanto que na área de segurança, exitem distintas formas de proteção dos dados baseadas na redundância, tais como espelhamento, replicação e paridade.
	\\
	
	Os pontos fortes da tecnologia RAID podem ser extrapoladas para um sistema de arquivos distribuídos a fim de suprir suas fragilidades sobre proteção de dados e problemas de confiabilidade.
	\\
