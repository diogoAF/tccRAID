
	\section{Contextualização}
	Tradicionalmente a ciência era dividida em três paradigmas: empírica, lógica e computacional, porém o cientista computacional Jim Gray idealizou um quarto chamado de \textit{data-intensive science}~\cite{hey2009}. Tal paradigma caracteriza-se pelo intenso processamento de \textit{big data} afim de conseguir informações úteis. \textit{Big data} consiste de um conjunto extremamente amplo de dados não estruturados. Justamente pela quantidade massiva de dados que compõe o \textit{big data}, é impraticável utilizar computadores convencionais para seu processamento.
	\\
	
	Atualmente a taxa de dados processados e armazenados globalmente aumenta anualmente. Tal crescimento não foi influenciado apenas pelos estudos científicos e grandes empresas, visto que grande parcela da população criou o abito de acessar diariamente as mais diversas modalidades de redes sociais, além de utilizar as facilidades da computação nas nuvens disponibilizada por empresas como o Dropbox ou a Google. Como exemplo da popularização de tais serviços, o \textit{Dropbox} divulgou possuir mais de 300.000.000 usuários cadastrados~\cite{dropbox} em 2014, onde cada usuários dispõem um espaço mínimo de 2GB para armazenar seus arquivos remotamente, desta forma, para suprir as necessidades de seus clientes, são necessários no mínimo 600PB de espaço de armazenamento. Sobre o ramo das redes sociais, existe o caso do \textit{Facebook}, considerada em 2012 como a mario em seu ramo de atuação, cujos usuários produzem cerca de 600TB diariamente~\cite{facebook14}. Enquanto que na área de pesquisas, A Organização Europeia para a Pesquisa Nuclear (\textit{The European Organization for Nuclear Research - CERN}), o maior laboratório de física de partículas do mundo, informou em 2013 que ao longo de 20 anos produziu 100PB de dados em seus experimentos~\cite{cern}.
	\\
	
	Os casos apresentados anteriormente demonstram o imenso volume de dados que são gerados, processados e armazenados diariamente em todo o globo. Nesses casos em que o volume de dados armazenados chega na ordem de \textit{petabytes} faz-se necessário buscar formas eficientes e seguras para salvar todas as informações. Por exemplo, utilizando-se discos rígidos de 2 TB, serão necessários 50.000 dispositivos para conseguir a capacidade total de 100 PB. Tamanha quantidade de equipamentos seria impossível de serem instaladas em um único servidor, o que acarreta na necessidade de se utilizar um sistema distribuído com vários servidores, ditos como nós, trabalhando conectados através de uma rede. Desta forma, é indispensável a utilização de um sistema de arquivos distribuído (SAD) para o gerenciamento de todos os arquivos espalhados entre todos os servidores. Como exemplos reais de SAD, temos  o \textit{Google File System}, desenvolvido e utilizado pela própria \textit{Google}, O \textit{Apache Hadoop}, utilizado pelo \textit{Facebook},além do \textit{Amazon S3}, utilizado pelo \textit{Dropbox}.
	\\
	
	Visto que em um SAD os arquivos estão armazenados distribuidamente entre vários nós, a falha de um nó pode inviabilizar o uso do sistema como um todo.
	
	% AQUI!!!!! 
	
	Quando um dos computadores fica indisponível ou apresenta algum problema grave, todos os arquivos contidos nele também ficam inacessíveis ou até sejam perdidos, resultando em grande prejuízo para sistema.
	Além disso, como a probabilidade de ocorrer falhas em algum dos componentes cresce proporcionalmente com o aumento da quantidade de componentes envolvidos, o incremento da escala resulta em redução da confiabilidade dos arquivos armazenados.
	Por isso, no SAD existe um mecanismo de proteção dos arquivos armazenados que é possível tolerar as falhas ocorridas em nós.
	A forma de proteção é a atribuição de redundância para cada um dos arquivos, criando algumas cópias de segurança chamadas de réplicas.
	A parte original do arquivo e cada uma das réplicas serão armazenados em computadores diferentes, assim, mesmo que um deles apresenta problema, o sistema ainda consegue acessar no arquivo, mantendo funcionamento normal.
	Como ocorre em \textit{Google File System}, em caso geral mantém 3 réplicas por configuração padrão, podendo ser alterada dependendo da necessidade.
	\\
	
A atribuição da redundância consegue impedir a queda da confiabilidade dos arquivos, mas isto tem um custo extra.
Se um arquivo é protegido com quantidade mínima de redundância, de só uma cópia, significa que no sistema existem dois dados idênticos, consumindo o 100\% de espaço a mais que somente o arquivo original ocuparia no dispositivo de armazenamento. 
Desta forma, manter os dados em larga escala sem degradar a segurança torna-se cada vez mais custoso com o aumento de volume total dos dados armazenados. 
Este aumento de custo causado pela necessidade do espaço extra é crucial quando trata de dados que possuem centenas de \textit{petabytes} de volume, como ocorre no caso de arquivos armazenados por um provedor de alguns serviços em rede ou no caso de um conjunto de dados considerado como \textit{big data}. \\

	
	\section{Justificativa}
	No contexto dos sistemas distribuídos o conceito de paralelismo é muito presente, o que acarreta no aumento de máquinas conectadas e consequentemente a probabilidade de alguma dessas máquinas sofrer alguma falha de \textit{hardware} ou \textit{software}. Quando essa falha ocorrer, é necessário que contra medidas tenham sido implementadas a fim de minimizar os danos. Uma dessas medidas é que o sistema de arquivos distribuído seja tolerante a falhas, tolerância essa que pode ser alcançada utilizando-se a replicação total dos dados, entretanto esse \textit{overhead} ocupa espaços de armazenamento que poderiam ser utilizados para novos dados. Por exemplo, na replicação tradicional de arquivo, o \textit{overhead} é de 100\%, pois para cada arquivo será criado uma cópia idêntica do mesmo.
	\\
	
	
	Para isso, nós observamos conceitos da tecnologia RAID, pois notamos alguns pontos semelhantes com o sistema de arquivos distribuídos, como o armazenamento de forma distribuído e a preocupação com a segurança dos arquivos.
	Além disso, a utilização da replicação de arquivo e de outro método para atribuição da redundância também foi motivo para colocar atenção nesta tecnologia.
	A outra forma de criar as partes redundantes é a geração de paridades, que são calculadas baseando-se em blocos divididos a partir do próprio arquivo.
	Dessa forma, é possível reduzir o \textit{overhead} de espaço até 50\%, a metade da replicação com uma cópia, quando as paridades forem geradas de forma mais simples, ou até menos, dependendo da condição utilizada para calcular a paridade.
	\\
	
	Também notamos o fato de que a tecnologia RAID foi inicialmente desenvolvida para aumentar a taxa de transferência da unidade de armazenamento.
	A motivação inicial desta invenção era reusar os discos rígidos que não estavam mais em uso, por serem substituídos por outros mais novos com melhor performance.
	O problema para fazer o reuso dos dispositivos era a baixa velocidade de leitura/escrita que estes apresentavam, por serem aparelhos relativamente velhos. 
	Para cobrir esta baixa velocidade de transferência, os discos foram colocados para trabalhar em conjunto, ou seja, fazer um acesso simultâneo em vários discos para aumentar a vazão total de dados transferidos, resultando em aumento da taxa de transferência .
	Assim, a introdução de conceito do RAID, não trata apenas de questão da segurança de arquivos, mas também no desempenho do sistema.  
	\\
	
	Para que o armazenamento de arquivo seja administrado de forma eficiente, contribuindo para aumento do desempenho do sistema, adotamos uma abordagem que separa o arquivo em duas partes para distribuir a carga de gerenciamento. 
	Uma parte são os metadados, que guardam as informações do arquivo, como nome, tamanho ou localização. E a outra parte é o dado, que armazena o próprio conteúdo deste arquivo. 
	Desta forma, o sistema será constituído por dois tipos diferentes de conjunto dos servidores, um que administra os metadados dos arquivos e outro que armazena os dados dos arquivos. 
	\\
	
	Da mesma forma que a confiabilidade do conteúdo dos arquivos é assegurado por conceito de RAID, os metadados armazenados também precisam ser protegidos.
	Porém, não podemos usar o RAID para fazer isso, pois nos servidores de metadados não acontecem simples entrada/saída de dados, mas também fornecem um serviço para indicar localização de conteúdo dos arquivos. 
	Por isso, para garantir a segurança dos metadados, junto com o serviço, utilizamos o \textit{BFT-SMaRt}, uma biblioteca em Java que disponibiliza as ferramentas para desenvolver um serviço tolerante a falhas, baseado na técnica de replicação de serviço.
	\\	
	
	Assim, o nosso trabalho está focalizado em construção de um sistema de arquivos distribuídos utilizando os conceitos de RAID.
	Será implementado algumas configurações de RAID, incluindo uma que utiliza a replicação de arquivo e outra que utiliza a geração de paridade, e posteriormente será feito a comparação entre as configurações desenvolvidas no sistema, para verificar a diferença de desempenho entre elas.
	\\

	
	
	
	

 

	

