
	\section{Contextualização}
	Atualmente, o volumes de dados que são processados e armazenados no mundo inteiro está crescendo cada vez mais, impulsionado intensamente com o aumento da capacidade de processamento e armazenamento de dados e a popularização da internet entre as pessoas. Na área da ciência surgiu um novo e quarto paradigma científico, seguindo de ciência empírica, ciência lógica e ciência computacional, chamado de \textit{data-intensive science}~\cite{hey2009}, proposto pelo cientista da computação pioneiro Jim Gray. Esse paradigma é caracterizado por focalizar na intensiva coleta e acúmulo de dados, para serem submetidos em análise a fim de extrair as informações vantajosas. Várias empresas e organizações estão dando atenção especial em \textit{big data}, um novo conceito que surgiu com aplicação do paradigma \textit{data-intensive science}, cujo consiste em conjunto de dados não estruturados extremamente amplo, e por esse motivo, é impossível de ser processado por um computador convencional. O \textit{big data} é usado para analisar as relações complexas que existem entre enorme quantidade de dados contidos nele para obter as informações como previsões ou tendencias, onde geralmente são impossíveis de serem obtidas se fosse usado um volume convencional de dados. Esse crescimento de volume de dados não afetou somente áreas de negócio ou cientifico, mas também os usuários finais, com foco em armazenamento de dados, evidenciado com aumento de capacidade do disco rígido contido em um computador pessoal ou aumento de demanda por um serviço de armazenamento remoto baseando em conceitos de computação em nuvem, chamado de \textit{cloud storage service}.
	\\ 

	%Atualmente, a importância de coleta e acúmulo de dados para submeter em análise a fim de extrair as informações vantajosas está sendo discutida em diversas áreas. Este fato é evidenciado com o surgimento de , e definição de novo conceito denominado de \textit{big data}, o conjunto de dados extremamente amplo, e por esse motivo, é impossível de ser processado por um computador convencional. \\
	
	Para ter noção concreta de tamanho de dados processados, apresentamos como exemplo alguns casos da utilização de \textit{big data}. O CERN, o maior laboratório de física de partículas do mundo, que tinha anunciado em 2013 que registrou mais de 100 petabytes de dados de pesquisa ao longo dos últimos 20 anos, em que 75 petabytes destes dados foram gerado com as experiências de colisão no \textit{Large Hadron Collider} (LHC), nos últimos 3 anos~\cite{cern}. Também temos o Facebook, um serviço de rede social que em 2012 foi considerado maior do mundo, possui um \textit{data warehouse} que armazena mais de 300 petabytes de dados acumulados, e ainda 600 terabytes de dados chegam a cada dia~\cite{facebook14}. No caso de Dropbox, o serviço \textit{cloud storage} mais pupular do mundo, oferece 2 gigabytes de espaço básico para todos os usuários, podendo ser aumentado até espaço ilimitado dependendo de plano, e registrou mais de 300.000.000 usuários cadastrados~\cite{dropbox}, implicando que é necessário manter o espaço de armazenamento no mínimo 600 petabytes disponível a qualquer instante para suprir todos seus usuários.\\
	
	Como mostra em exemplos apresentados, um volume imenso de dados estão sendo intensamente coletado e submetido a análise. No caso do Dropbox que possui um sistema com 600 petabytes de capacidade de armazenamento estimada, implicando que será necessário, quando for usado um disco rígido com 2 terabytes de capacidade na sua composição, utilizar 300.000 dispositivos, sendo extremamente difícil de ser equipados em uma máquina normal. Existe também problema com gerenciamento de dados, pois nenhum computador convencional possui o desempenho suficiente para tratamento de dados nesta escala. 
	
	
	Uma solução inventada para  resolver a questão de armazenamento de dados é aumentar igualmente a escala do sistema, realizado com a incorporação de um sistema de arquivo distribuído, que possui algumas vantagens como facilidade e menor custo para aumento de desempenho, comparado a outras abordagens. Consiste de um conjunto de vários computadores ou servidores fisicamente distribuídos, chamados de nós, e todos eles conectados através de uma rede. Contudo, em uma abordagem de armazenamento distribuído muitas vezes é  acompanhado com problema de degradação da confiabilidade de conteúdos armazenados, onde o risco de perder alguns arquivos cresce cada vez que aumenta o tamanho do sistema, pois eles estão localizados distribuidamente sobre vários computadores, e a probabilidade de ocorrer uma falha em algum dos componentes incrementa proporcionalmente com número de entidades envolvidos. Para tratar esse problema de confiabilidade, o sistema de arquivos distribuído possui esquema de tolerar as falhas, atribuindo a redundância nos arquivos, e a maioria dos casos usam a criação de algumas cópias de segurança dos arquivos armazenados, chamadas de réplicas. Em geral, são mantidos três ou mais réplicas para cada arquivo, incluindo a parte original, o que resulta em maior consumo de espaço que deveria, necessitando de no mínimo três vezes a mais espaço de armazenamento que o espaço realmente ocupado por todos os arquivos.
	
	Assim, manter a grande quantidade de arquivos com segurança está tornando-se cada vez mais custoso com aumento de volume dos dados armazenados, ainda mais quando a ordem de tamanho chega em centenas de petabytes, como ocorre no caso de um \textit{big data} ou todos os arquivos encontrados em armazenamento nas nuvens. 
	%Para lidar com essa complexidade, uma opção é o uso da abordagem de \textit{State Machine Replication} (SMR), ou a replicação de máquina de estado, que possui o objetivo de aumentar o desempenho e a capacidade de sistema ou de prover tolerância a falha~\cite{alchieri}, criando as réplicas de computador ou servidor.\\
	
	%O uso do sistema de arquivos distribuídos resolve a questão de capacidade para armazenamento de dados de grande porte com incremento de escala, acrescentando um servidor quando for necessário. Este método é bem eficiente, prático e relativamente barato, contudo pode se tornar custoso, pois não ocorre somente adicionamento das máquinas, mas também a substituição, seja parcial ou inteira, dos componentes que apresentou algum defeito, e certamente isso ocorre com uma frequência significante e ser intenso cada vez que aumenta a escala, acumulando o custo ao longo do tempo.\\
	

	
	
	
	

 

	

